{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "523dd53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\rasman khurshid\\big data processing\\big-data-processing-spark-ml-pipeline\\.venv\\lib\\site-packages (4.1.1)\n",
      "Requirement already satisfied: findspark in c:\\users\\rasman khurshid\\big data processing\\big-data-processing-spark-ml-pipeline\\.venv\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in c:\\users\\rasman khurshid\\big data processing\\big-data-processing-spark-ml-pipeline\\.venv\\lib\\site-packages (from pyspark) (0.10.9.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!\"{sys.executable}\" -m pip install pyspark findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116f0de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataWarehouseAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14743126",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df96548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ['DimAccount', 'DimCurrency', 'DimCustomer', 'DimDate', 'DimDepartmentGroup', 'DimGeography', 'DimOrganization', 'DimProduct', 'DimProductCategory', 'DimProductSubcategory', 'DimPromotion', 'DimReseller', 'DimSalesReason', 'DimSalesTerritory', 'DimScenario', 'FactCallCenter', 'FactCurrencyRate', 'FactFinance', 'FactInternetSales', 'FactSalesTargets']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Portable: repo_root/data  (works on any machine)\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_DIR = Path(os.getenv(\"DATA_DIR\", str(PROJECT_ROOT / \"data\")))\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"DATA_DIR not found: {DATA_DIR}\\n\"\n",
    "        f\"Put CSV files in: {PROJECT_ROOT/'data'}\\n\"\n",
    "        f\"Or set an environment variable DATA_DIR to your dataset folder.\"\n",
    "    )\n",
    "\n",
    "# List all CSV files in the data folder\n",
    "csv_files = [p for p in DATA_DIR.iterdir() if p.suffix.lower() == \".csv\"]\n",
    "\n",
    "dataframes = {}\n",
    "for p in csv_files:\n",
    "    df_name = p.stem  # filename without .csv\n",
    "    dataframes[df_name] = spark.read.csv(str(p), header=True, inferSchema=True)\n",
    "\n",
    "print(\"Loaded:\", list(dataframes.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a134e",
   "metadata": {},
   "source": [
    "**Data Cleaning** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e2fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "missing_values = {}\n",
    "for df_name, df in dataframes.items():\n",
    "    # Count missing values for each column\n",
    "    missing_count = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()\n",
    "    missing_values[df_name] = {col: val for col, val in zip(df.columns, missing_count[0])}\n",
    "\n",
    "# The dictionary 'missing_values' will have the count of missing values for each column in each DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c3cc94",
   "metadata": {},
   "source": [
    "**Missing Values in each DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eec458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "missing_values = {}\n",
    "for df_name, df in dataframes.items():\n",
    "    # Count missing values for each column\n",
    "    missing_count = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()\n",
    "    missing_values[df_name] = {col: val for col, val in zip(df.columns, missing_count[0])}\n",
    "\n",
    "# The dictionary 'missing_values' will have the count of missing values for each column in each DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd17288",
   "metadata": {},
   "source": [
    "**Dropping Unnecessary Columns from Customer Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b20222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dropping 'MiddleName', 'AddressLine2' and 'Title' columns from 'DimCustomer'\n",
    "dimCustomer = dataframes['DimCustomer']\n",
    "dimCustomer = dimCustomer.drop('MiddleName', 'AddressLine2', 'Title', 'Suffix')\n",
    "\n",
    "# Update the DataFrame dictionary\n",
    "dataframes['DimCustomer'] = dimCustomer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a9f663",
   "metadata": {},
   "source": [
    "**Handling Outliers** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f27171aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: YearlyIncome\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 309\n",
      "\n",
      "Column: TotalChildren\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 0\n",
      "\n",
      "Column: NumberChildrenAtHome\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 3260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, percentile_approx, col\n",
    "\n",
    "dimCustomer = dataframes['DimCustomer']\n",
    "\n",
    "# List of numerical columns to check for outliers\n",
    "numerical_columns = ['YearlyIncome', 'TotalChildren', 'NumberChildrenAtHome'] \n",
    "\n",
    "outliers = {}\n",
    "for col_name in numerical_columns:\n",
    "    # Calculate the lower and upper quartiles\n",
    "    bounds = dimCustomer.stat.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
    "    IQR = bounds[1] - bounds[0]\n",
    "\n",
    "    # Define lower and upper bounds for outliers\n",
    "    lower_bound = bounds[0] - 1.5 * IQR\n",
    "    upper_bound = bounds[1] + 1.5 * IQR\n",
    "\n",
    "    # Count outliers\n",
    "    outliers_below = dimCustomer.filter(col(col_name) < lit(lower_bound)).count()\n",
    "    outliers_above = dimCustomer.filter(col(col_name) > lit(upper_bound)).count()\n",
    "\n",
    "    # Store the count of outliers\n",
    "    outliers[col_name] = {'below': outliers_below, 'above': outliers_above}\n",
    "\n",
    "# Print the results\n",
    "for col_name, counts in outliers.items():\n",
    "    print(f\"Column: {col_name}\")\n",
    "    print(f\"  Outliers below lower bound: {counts['below']}\")\n",
    "    print(f\"  Outliers above upper bound: {counts['above']}\\n\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c4d97",
   "metadata": {},
   "source": [
    "The 'YearlyIncome' and 'NumberChildrenAtHome' fields have a significant number of outliers above their respective upper bounds. This suggests that there are customers with unusually high yearly incomes and a number of children at home compared to the general population of the dataset. The absence of outliers for 'TotalChildren' suggests that the distribution of this variable is relatively tight, with most data points falling within a normal range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ff05b4",
   "metadata": {},
   "source": [
    "**Handling Outliers through Capping in DimCustomer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05e414e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Cap outliers\n",
    "dimCustomer = dimCustomer.withColumn(col_name, when(col(col_name) < lit(lower_bound), lit(lower_bound))\n",
    "                                               .when(col(col_name) > lit(upper_bound), lit(upper_bound))\n",
    "                                               .otherwise(col(col_name)))\n",
    "\n",
    "# Update the DataFrame in dictionary\n",
    "dataframes['DimCustomer'] = dimCustomer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe51b39",
   "metadata": {},
   "source": [
    "**Counting Outliers again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75199c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: YearlyIncome\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 309\n",
      "\n",
      "Column: TotalChildren\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 0\n",
      "\n",
      "Column: NumberChildrenAtHome\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dimCustomer = dataframes['DimCustomer']\n",
    "\n",
    "# List of numerical columns to check for outliers\n",
    "numerical_columns = ['YearlyIncome', 'TotalChildren', 'NumberChildrenAtHome'] \n",
    "\n",
    "outliers = {}\n",
    "for col_name in numerical_columns:\n",
    "    # Calculate the lower and upper quartiles\n",
    "    bounds = dimCustomer.stat.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
    "    IQR = bounds[1] - bounds[0]\n",
    "\n",
    "    # Define lower and upper bounds for outliers\n",
    "    lower_bound = bounds[0] - 1.5 * IQR\n",
    "    upper_bound = bounds[1] + 1.5 * IQR\n",
    "\n",
    "    # Count outliers\n",
    "    outliers_below = dimCustomer.filter(col(col_name) < lit(lower_bound)).count()\n",
    "    outliers_above = dimCustomer.filter(col(col_name) > lit(upper_bound)).count()\n",
    "\n",
    "    # Store the count of outliers\n",
    "    outliers[col_name] = {'below': outliers_below, 'above': outliers_above}\n",
    "\n",
    "# Print the results\n",
    "for col_name, counts in outliers.items():\n",
    "    print(f\"Column: {col_name}\")\n",
    "    print(f\"  Outliers below lower bound: {counts['below']}\")\n",
    "    print(f\"  Outliers above upper bound: {counts['above']}\\n\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9be10",
   "metadata": {},
   "source": [
    "The output shows that in the 'DimCustomer' dataset, 'YearlyIncome' has 309 outliers above the upper bound, while 'TotalChildren' and 'NumberChildrenAtHome' have no outliers either below the lower bound or above the upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2e0df",
   "metadata": {},
   "source": [
    "**Checking Outliers in FactInternetSales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c1f1333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: OrderQuantity\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 0\n",
      "\n",
      "Column: UnitPrice\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 15454\n",
      "\n",
      "Column: ExtendedAmount\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 15454\n",
      "\n",
      "Column: SalesAmount\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 15454\n",
      "\n",
      "Column: TaxAmt\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 15454\n",
      "\n",
      "Column: Freight\n",
      "  Outliers below lower bound: 0\n",
      "  Outliers above upper bound: 15454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "factInternetSales = dataframes['FactInternetSales']\n",
    "\n",
    "# List of numerical columns to check for outliers\n",
    "numerical_columns1 = ['OrderQuantity', 'UnitPrice', 'ExtendedAmount', 'SalesAmount', 'TaxAmt', 'Freight'] \n",
    "\n",
    "outliers = {}\n",
    "for col_name in numerical_columns1:\n",
    "    # Calculate the lower and upper quartiles\n",
    "    bounds = factInternetSales.stat.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
    "    IQR = bounds[1] - bounds[0]\n",
    "\n",
    "    # Define lower and upper bounds for outliers\n",
    "    lower_bound = bounds[0] - 1.5 * IQR\n",
    "    upper_bound = bounds[1] + 1.5 * IQR\n",
    "\n",
    "    # Count outliers\n",
    "    outliers_below = factInternetSales.filter(col(col_name) < lit(lower_bound)).count()\n",
    "    outliers_above = factInternetSales.filter(col(col_name) > lit(upper_bound)).count()\n",
    "\n",
    "    # Store the count of outliers\n",
    "    outliers[col_name] = {'below': outliers_below, 'above': outliers_above}\n",
    "\n",
    "# Print the results\n",
    "for col_name, counts in outliers.items():\n",
    "    print(f\"Column: {col_name}\")\n",
    "    print(f\"  Outliers below lower bound: {counts['below']}\")\n",
    "    print(f\"  Outliers above upper bound: {counts['above']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5ae92",
   "metadata": {},
   "source": [
    "**Handling Outliers in FactInternetSales using Segmentation:**\n",
    "    High values in FactInternetSales (like UnitPrice, SalesAmount, etc.) may represent large, legitimate transactions (e.g., bulk purchases, high-value items). Removing these outliers might lead to losing insights into a potentially significant customer segment. Segmenting sales data into different categories, such as regular sales and high-value sales allows for more nuanced analysis and understanding of different customer behaviors. I will perform segmentation later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8af35e",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis (EDA) of DimCustomer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc8714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "dim_customer_df = dataframes['DimCustomer']\n",
    "# Convert PySpark DataFrame to Pandas DataFrame for tabular view\n",
    "pandas_dim_customer_df = dim_customer_df.toPandas()\n",
    "\n",
    "# Display the first few rows in a tabular format\n",
    "pandas_dim_customer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06aeea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dim_customer_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dim_customer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaca795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecting specific columns to create a new DataFrame\n",
    "new_df = pandas_dim_customer_df[['YearlyIncome', 'TotalChildren', 'NumberChildrenAtHome']]\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 16))\n",
    "sns.pairplot(new_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24793e9d",
   "metadata": {},
   "source": [
    "**Here are the observations:**\n",
    "\n",
    "**'YearlyIncome' Distribution:** The histogram for 'YearlyIncome' indicates a right-skewed distribution, suggesting that most customers have a lower income, with fewer customers earning higher incomes.\n",
    "\n",
    "**'TotalChildren' Distribution:** The 'TotalChildren' histogram shows a relatively uniform distribution for lower counts of children, but it decreases as the number of children increases, indicating fewer customers have a larger number of children.\n",
    "\n",
    "**'NumberChildrenAtHome' Distribution:** The histogram for 'NumberChildrenAtHome' indicates that a majority of customers have no children at home or a small number, with the frequency decreasing as the number of children at home increases.\n",
    "\n",
    "**'YearlyIncome' vs. 'TotalChildren':** The scatter plot does not indicate a clear correlation between 'YearlyIncome' and 'TotalChildren'. There is a wide spread of incomes for customers with few children, and as the number of children increases, the range of incomes seems to narrow, but this could be due to fewer data points.\n",
    "\n",
    "**'YearlyIncome' vs. 'NumberChildrenAtHome':** Similar to 'TotalChildren', there is no apparent correlation between 'YearlyIncome' and 'NumberChildrenAtHome'. Incomes are spread across the range for customers with few children at home.\n",
    "\n",
    "**'TotalChildren' vs. 'NumberChildrenAtHome':** The scatter plot shows a pattern where as the 'TotalChildren' increases, 'NumberChildrenAtHome' also tends to increase, which is an expected relationship. However, there are many instances where customers have more 'TotalChildren' but none or fewer 'NumberChildrenAtHome', which may indicate children who have grown up and moved out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = 'C:/Users/rasman khurshid/Desktop/Data Processing/Dataset_Final/DataSet_final/DimCustomer.csv'\n",
    "\n",
    "# Try different encodings if 'utf-8' doesn't work\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='latin1')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file_path, encoding='iso-8859-1')  # or try 'cp1252'\n",
    "        \n",
    "print(sorted(df['NumberChildrenAtHome'].unique()))\n",
    "\n",
    "# Convert the 'NumberChildrenAtHome' from float to int\n",
    "df['NumberChildrenAtHome'] = df['NumberChildrenAtHome'].astype(int)\n",
    "\n",
    "# Now check the unique values again\n",
    "print(sorted(df['NumberChildrenAtHome'].unique()))\n",
    "\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Increase the figure size to make the plot wider\n",
    "plt.figure(figsize=(10, 12))  # Adjust the size as needed\n",
    "\n",
    "# Create a bar plot for 'NumberChildrenAtHome' column\n",
    "ax = sns.countplot(x='NumberChildrenAtHome', data=df)\n",
    "\n",
    "# Set the title and labels of the plot\n",
    "ax.set_title('Count of Number of Children at Home')\n",
    "ax.set_xlabel('Number of Children at Home')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f843f1e5",
   "metadata": {},
   "source": [
    "## Distribution of Total Children Across Yearly Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7db571",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.violinplot(x='YearlyIncome', y='TotalChildren', data=pandas_dim_customer_df)\n",
    "plt.title('Distribution of Total Children Across Yearly Income Ranges')\n",
    "plt.xlabel('Yearly Income')\n",
    "plt.ylabel('Total Children')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8ae8d",
   "metadata": {},
   "source": [
    "**Analysis:** The violin plot visualizes the distribution of 'Total Children' across various 'Yearly Income' ranges. Each violin shape corresponds to a different income bracket, showing the distribution of family sizes within that bracket. The plot suggests a relatively consistent median of around 1 to 2 children across all income ranges, as indicated by the white dot within each violin. The thickness of each violin indicates the density of data points at different counts of 'Total Children', with the thickest part representing the most common value. Notably, there are no significant outliers or extreme variations in family size across different income levels, as all violins maintain a similar shape and width throughout their length. The consistency across the income ranges suggests there is no strong correlation between the number of children and yearly income within this dataset. The range of 'Total Children' is mostly between 0 and 3 for all income levels, indicating that most families in this dataset have fewer than 3 children, regardless of income.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de98b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x='YearlyIncome', y='TotalChildren', data=pandas_dim_customer_df)\n",
    "plt.title('Box Plot of Total Children Across Yearly Income Ranges')\n",
    "plt.xlabel('Yearly Income')\n",
    "plt.ylabel('Total Children')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e7c41",
   "metadata": {},
   "source": [
    "**Age Distribution of Customers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87686b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convert BirthDate from string to datetime\n",
    "pandas_dim_customer_df['BirthDate'] = pd.to_datetime(pandas_dim_customer_df['BirthDate'])\n",
    "\n",
    "# Calculate age\n",
    "current_year = datetime.now().year\n",
    "pandas_dim_customer_df['Age'] = current_year - pandas_dim_customer_df['BirthDate'].dt.year\n",
    "# Convert BirthDate from string to datetime\n",
    "pandas_dim_customer_df['BirthDate'] = pd.to_datetime(pandas_dim_customer_df['BirthDate'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(pandas_dim_customer_df['Age'], bins=20, kde=True)\n",
    "plt.title('Age Distribution of Customers')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989bd67",
   "metadata": {},
   "source": [
    "**Histogram Analysis:** \n",
    "The distribution of age appears to be right-skewed, meaning there are more younger customers than older ones.\n",
    "The bulk of the customers seem to fall within the range of approximately 50 to 70 years old.\n",
    "There is a long tail extending into the older age ranges, suggesting a smaller number of much older customers. The skew towards younger ages suggests a potentially significant segment of the market. Marketing strategies targeting this demographic could focus on products or services that appeal to a younger audience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14321d3",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis (EDA) for DimGeography**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_geo_df = dataframes['DimGeography']\n",
    "# Convert PySpark DataFrame to Pandas DataFrame for tabular view\n",
    "pandas_dim_geo_df = dim_geo_df.toPandas()\n",
    "\n",
    "# Display the first few rows in a tabular format\n",
    "pandas_dim_geo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dim_geo_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb05862",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dim_geo_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9fa48f",
   "metadata": {},
   "source": [
    "**Understanding how customers are distributed geographically**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Distribution by City\n",
    "city_counts = pandas_dim_geo_df['City'].value_counts()\n",
    "# Limit to top N cities for better readability\n",
    "top_cities = city_counts.head(20)  # Adjust N as needed\n",
    "\n",
    "plt.figure(figsize=(14, 8))  # Increase figure size\n",
    "sns.barplot(y=top_cities.index, x=top_cities.values, palette='viridis')  # Horizontal bar chart\n",
    "plt.title('Top 20 Cities by Number of Customers')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('City')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7bfb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by StateProvinceName\n",
    "state_counts = pandas_dim_geo_df['StateProvinceName'].value_counts()\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=state_counts.index, y=state_counts.values, palette='viridis') \n",
    "plt.title('Distribution of Customers by State/Province')\n",
    "plt.xlabel('State/Province')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=90)  \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e715c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Distribution by CountryRegionCode\n",
    "country_counts = pandas_dim_geo_df['CountryRegionCode'].value_counts()\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=country_counts.index, y=country_counts.values, palette='viridis') \n",
    "plt.title('Distribution of Customers by Country')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xticks(rotation=45) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d6c7e",
   "metadata": {},
   "source": [
    "**Analysis:** Together, these plots provide an understanding of where the customer base is concentrated. The US dominates the customer distribution, with significant counts in key European cities and Canadian provinces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bf629",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis (EDA) for DimSalesTerritory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3de495",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_saleTerritory_df = dataframes['DimSalesTerritory']\n",
    "# Convert PySpark DataFrame to Pandas DataFrame for tabular view\n",
    "pandas_dim_saleTerritory_df = dim_saleTerritory_df.toPandas()\n",
    "\n",
    "# Display the first few rows in a tabular format\n",
    "pandas_dim_saleTerritory_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dim_saleTerritory_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30713b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dim_saleTerritory_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f3eb3",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis (EDA) for FactInternetSales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c06f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_factInternet_df = dataframes['FactInternetSales']\n",
    "# Convert PySpark DataFrame to Pandas DataFrame for tabular view\n",
    "pandas_dim_factInternet_df = dim_factInternet_df.toPandas()\n",
    "\n",
    "# Display the first few rows in a tabular format\n",
    "pandas_dim_factInternet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dim_factInternet_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a581bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dim_factInternet_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb564ec",
   "metadata": {},
   "source": [
    "## Sales Performance by Sales Territoty Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82310c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the sales data with the sales territory data on SalesTerritoryKey\n",
    "merged_df = pd.merge(pandas_dim_factInternet_df, pandas_dim_saleTerritory_df, on='SalesTerritoryKey', how='left')\n",
    "\n",
    "# Aggregate sales by territory\n",
    "sales_by_territory = merged_df.groupby('SalesTerritoryRegion')['SalesAmount'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Convert the aggregated data to a DataFrame for visualization\n",
    "sales_by_territory_df = sales_by_territory.reset_index()\n",
    "\n",
    "# Visualize sales performance by territory\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x='SalesTerritoryRegion', y='SalesAmount', data=sales_by_territory_df)\n",
    "plt.title('Sales Performance by Sales Territory Region')\n",
    "plt.xlabel('Sales Territory Region')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97334ead",
   "metadata": {},
   "source": [
    "A DataFrame merged_df is created by merging sales data with sales territory data on the 'SalesTerritoryKey'.\n",
    "The data is then grouped by 'SalesTerritoryRegion' and the sales amounts are summed.\n",
    "The aggregated sales data is sorted in descending order and reset into a new DataFrame for visualization purposes.\n",
    "A bar plot is created with Seaborn, showing 'SalesTerritoryRegion' on the x-axis and 'SalesAmount' on the y-axis.\n",
    "The title, axis labels, and rotation of the x-axis tick labels are set to make the chart more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart to visualize the sales performance by region\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sales_by_region, labels=sales_by_region.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Sales Performance by Sales Territory Region')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "# Display the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256719c1",
   "metadata": {},
   "source": [
    "These charts are useful for quickly identifying which regions are leading in sales and which are lagging, which can inform strategic business decisions such as where to allocate resources or where to implement changes to improve sales performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654dfafc",
   "metadata": {},
   "source": [
    "**Handling Outliers in FactInternetSales using Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaeede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, lit, count\n",
    "\n",
    "factInternetSales = dataframes['FactInternetSales']\n",
    "\n",
    "# Determine the threshold for high-value sales, e.g., 75th percentile of SalesAmount\n",
    "threshold = factInternetSales.approxQuantile('SalesAmount', [0.75], 0.05)[0]\n",
    "\n",
    "# Create a new column for segment identification\n",
    "factInternetSales = factInternetSales.withColumn('SaleType', \n",
    "                                                 when(col('SalesAmount') > lit(threshold), lit('High-Value'))\n",
    "                                                 .otherwise(lit('Regular')))\n",
    "\n",
    "# Update the DataFrame in dictionary\n",
    "dataframes['FactInternetSales'] = factInternetSales\n",
    "# Group by the 'SaleType' and count the number of occurrences in each group\n",
    "segmentation_count = factInternetSales.groupBy('SaleType').agg(count('*').alias('Count'))\n",
    "\n",
    "factInternetSalesPandas = factInternetSales.toPandas()\n",
    "factInternetSalesPandas.head()\n",
    "segmentation_count_pandas = segmentation_count.toPandas()\n",
    "print(segmentation_count_pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c208d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart for Segment Counts\n",
    "segmentation_count_pandas.plot(kind='bar', x='SaleType', y='Count')\n",
    "plt.xlabel('Sale Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Sales by Type')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728fd34",
   "metadata": {},
   "source": [
    "# Predicting High-Value Customers using Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2232a",
   "metadata": {},
   "source": [
    "Objective: To identify and predict high-value customers based on their purchase history, demographics, and geographic information.\n",
    "\n",
    "Rationale: This use case is important because it helps the business focus its marketing and customer service efforts on individuals who are likely to generate the most revenue. By identifying high-value customers, the company can optimize resource allocation and tailor its strategies to retain these key customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f09bbd",
   "metadata": {},
   "source": [
    "**Performing Joins** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ef209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customer_geo_joined = dataframes['DimCustomer'].join(dataframes['DimGeography'], 'GeographyKey', 'left')\n",
    "\n",
    "sales_customer_joined = dataframes['FactInternetSales'].join(customer_geo_joined, 'CustomerKey', 'inner')\n",
    "final_joined_df = sales_customer_joined.join(dataframes['DimSalesTerritory'], 'SalesTerritoryKey', 'left')\n",
    "# Join DimCustomer with DimGeography\n",
    "customer_geo_joined = dataframes['DimCustomer'].join(dataframes['DimGeography'], 'GeographyKey', 'left')\n",
    "\n",
    "# Join FactInternetSales with the combined DimCustomer and DimGeography\n",
    "sales_customer_joined = dataframes['FactInternetSales'].join(customer_geo_joined, 'CustomerKey', 'inner')\n",
    "\n",
    "# Join the result with DimSalesTerritory\n",
    "final_joined_df = sales_customer_joined.join(dataframes['DimSalesTerritory'], 'SalesTerritoryKey', 'left')\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate the average sales amount by sales territory\n",
    "avg_sales_by_territory = factInternetSales.groupBy('SalesTerritoryKey').agg(\n",
    "    F.avg('SalesAmount').alias('AvgSalesAmount'),\n",
    "    F.sum(F.when(col('SaleType') == 'High-Value', 1).otherwise(0)).alias('HighValueSalesCount'),\n",
    "    F.count('*').alias('TotalSalesCount')\n",
    ")\n",
    "\n",
    "# Calculate the proportion of high-value sales\n",
    "avg_sales_by_territory = avg_sales_by_territory.withColumn(\n",
    "    'HighValueSalesProportion', \n",
    "    col('HighValueSalesCount') / col('TotalSalesCount')\n",
    ")\n",
    "\n",
    "# Join this data back to the main DataFrame\n",
    "final_joined_df = final_joined_df.join(avg_sales_by_territory, 'SalesTerritoryKey', 'left')\n",
    "# Drop the duplicate column after the join\n",
    "# Drop one of the duplicate 'SalesTerritoryKey' columns\n",
    "final_joined_df = final_joined_df.drop(\"SalesTerritoryKey\")\n",
    "\n",
    "final_joined_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b305ffa0",
   "metadata": {},
   "source": [
    "**Data Cleaning** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ebc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values = final_joined_df.select([count(when(col(c).isNull(), c)).alias(c) for c in final_joined_df.columns]).collect()\n",
    "print(missing_values)\n",
    "\n",
    "final_joined_df = final_joined_df.dropDuplicates()\n",
    "\n",
    "final_joined_df = final_joined_df.withColumn(\"NumberChildrenAtHome\", col(\"NumberChildrenAtHome\").cast(\"int\"))\n",
    "# Print the data types of each column\n",
    "final_joined_df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0f4e5",
   "metadata": {},
   "source": [
    "**Checking for any possible data leakage** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892883d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "final_joined_df = final_joined_df.withColumn(\"SaleTypeNumeric\", when(col(\"SaleType\") == \"High-Value\", 1).otherwise(0))\n",
    "\n",
    "# List of numerical features to check for correlation\n",
    "numerical_features = ['YearlyIncome', 'TotalChildren', 'NumberChildrenAtHome', 'OrderQuantity', 'UnitPrice'] \n",
    "# Calculate correlations and collect them into a dictionary\n",
    "correlations = {}\n",
    "for feature in numerical_features:\n",
    "    correlation = final_joined_df.stat.corr(\"SaleTypeNumeric\", feature)\n",
    "    correlations[feature] = correlation\n",
    "\n",
    "# Print out correlations\n",
    "for feature, correlation in correlations.items():\n",
    "    print(f\"Correlation between SaleType and {feature}: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78abd920",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9f72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator  # Importing the evaluator\n",
    "\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_cols = ['MaritalStatus', 'Gender', 'EnglishEducation', 'City', 'StateProvinceName', 'CountryRegionCode', 'SalesTerritoryRegion']\n",
    "numerical_cols = ['YearlyIncome', 'TotalChildren', 'NumberChildrenAtHome', 'OrderQuantity', 'UnitPrice']\n",
    "\n",
    "# StringIndexer for categorical columns (including target variable 'SaleType')\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_indexed\").fit(final_joined_df) for c in categorical_cols]\n",
    "indexers.append(StringIndexer(inputCol='SaleType', outputCol='label').fit(final_joined_df))\n",
    "\n",
    "# OneHotEncoder for indexed categorical columns\n",
    "encoders = [OneHotEncoder(inputCols=[c+\"_indexed\"], outputCols=[c+\"_encoded\"]) for c in categorical_cols]\n",
    "\n",
    "# VectorAssembler to create feature vector\n",
    "assemblerInputs = [c+\"_encoded\" for c in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Set up the Pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, lr])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(train, test) = final_joined_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate the model using Area Under ROC\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"Area Under ROC: {roc_auc}\")\n",
    "\n",
    "# Optionally, I can also evaluate using Area Under Precision-Recall Curve\n",
    "evaluator.setMetricName(\"areaUnderPR\")\n",
    "pr_auc = evaluator.evaluate(predictions)\n",
    "print(f\"Area Under Precision-Recall Curve: {pr_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553919c",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "predictions_pd = predictions.select(\"label\", \"prediction\").toPandas()\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(predictions_pd['label'], predictions_pd['prediction'])\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(predictions_pd['label'], predictions_pd['prediction'])\n",
    "precision = precision_score(predictions_pd['label'], predictions_pd['prediction'])\n",
    "recall = recall_score(predictions_pd['label'], predictions_pd['prediction'])\n",
    "f1 = f1_score(predictions_pd['label'], predictions_pd['prediction'])\n",
    "\n",
    "# Print Confusion Matrix and Metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea02c11",
   "metadata": {},
   "source": [
    "**Confusion Matrix:**\n",
    "\n",
    "True Negatives (TN): 12775 (cases correctly predicted as negative)\n",
    "False Positives (FP): 1 (cases incorrectly predicted as positive)\n",
    "False Negatives (FN): 23 (cases incorrectly predicted as negative)\n",
    "True Positives (TP): 5132 (cases correctly predicted as positive)\n",
    "Performance Metrics:\n",
    "\n",
    "Accuracy: Approximately 99.87%, indicating that the model correctly predicted the label for nearly all of the test data.\n",
    "Precision: Approximately 99.98%, showing that almost all of the instances predicted as positive are indeed positive.\n",
    "Recall: Approximately 99.55%, indicating that the model successfully identified most of the positive cases.\n",
    "F1 Score: Approximately 99.77%, which is the harmonic mean of precision and recall, also indicating a very high performance.\n",
    "These results suggest that the model is highly effective at classifying the test data with both high precision and high recall, which is often a challenging balance to achieve in practice. The model successfully identifies positive cases while maintaining a low rate of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5567dee",
   "metadata": {},
   "source": [
    "**Plotting Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "predictions_pd = predictions.toPandas()\n",
    "\n",
    "# Set the size of the plot for better visibility\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Create a subplot for High-Value probabilities\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(predictions_pd['probability'].apply(lambda x: x[1]), bins=20, alpha=0.5, color='blue')\n",
    "plt.title('Probability Distribution for High-Value')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Create a subplot for Regular probabilities\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predictions_pd['probability'].apply(lambda x: x[0]), bins=20, alpha=0.5, color='red')\n",
    "plt.title('Probability Distribution for Regular')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot with a tight layout to ensure no overlap of subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10237d",
   "metadata": {},
   "source": [
    "**From the histograms, we can see that:**\n",
    "\n",
    "The 'High-Value' probabilities (on the left) have a concentration of frequencies at the lower probability values and another concentration at the higher probability values. This bimodal distribution suggests that the model is quite certain about a portion of the 'High-Value' predictions, while another portion is predicted with low probability.\n",
    "The 'Regular' probabilities (on the right) are predominantly concentrated at the higher probability values. This indicates that the model is very confident in predicting regular sales, with most of the predicted probabilities close to 1.\n",
    "These histograms provide a much clearer separation of the two predicted classes and can help in understanding the model's confidence in its predictions. It appears that the model is very confident in predicting 'Regular' sales, while it has a spread of confidence levels for 'High-Value' sales. This could be indicative of the model having a better grasp on the characteristics that define 'Regular' sales compared to 'High-Value' sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f64815",
   "metadata": {},
   "source": [
    "# Predicting Number of Car Owned Using Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e153b",
   "metadata": {},
   "source": [
    "**Objective:** To predict Number of Cars Owned by customers based on their yearly income and demographics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3476a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/rasman khurshid/Desktop/Data Processing/Dataset_Final/DataSet_final/DimCustomer.csv'\n",
    "dim_customer_df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Encoding 'CommuteDistance' as an ordinal variable based on its unique values\n",
    "commute_distance_mapping = {\n",
    "    '0-1 Miles': 1,\n",
    "    '1-2 Miles': 2,\n",
    "    '2-5 Miles': 3,\n",
    "    '5-10 Miles': 4,\n",
    "    '10+ Miles': 5\n",
    "}\n",
    "dim_customer_df['CommuteDistanceEncoded'] = dim_customer_df['CommuteDistance'].map(commute_distance_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38daebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Correlation Matrix and Heatmap\n",
    "# Relevant columns for EDA\n",
    "eda_columns = ['YearlyIncome', 'HouseOwnerFlag', 'TotalChildren', \n",
    "               'NumberChildrenAtHome', 'CommuteDistanceEncoded', 'NumberCarsOwned']\n",
    "eda_df = dim_customer_df[eda_columns]\n",
    "correlation_matrix = eda_df.corr()\n",
    "#plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix of Selected Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c948f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the correlation matrix in a table format\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8fc33",
   "metadata": {},
   "source": [
    "**Analysis**: The correlation matrix and heatmap provide a visual and numerical representation of the relationship between various features and the target variable 'NumberCarsOwned'. The heatmap uses color intensity to indicate the strength and direction of the correlation, where red tones represent positive correlations and blue tones represent negative correlations.\n",
    "\n",
    "From the analysis, 'YearlyIncome' shows the strongest positive correlation (0.47) with the target variable, suggesting that as income increases, the number of cars owned tends to increase as well. 'NumberChildrenAtHome' also has a strong positive correlation (0.42) with 'NumberCarsOwned', which implies that households with more children at home may require more cars. 'CommuteDistanceEncoded' has a moderate positive correlation (0.42) with the target, indicating that longer commute distances might be associated with owning more cars. On the other hand, 'HouseOwnerFlag' has a negligible negative correlation (-0.06) with 'NumberCarsOwned', hinting that home ownership status might not be a significant predictor of car ownership in this dataset.\n",
    "\n",
    "The correlation between 'YearlyIncome' and 'NumberCarsOwned' is particularly noteworthy as it suggests a potential avenue for predictive modeling. While correlation does not imply causation, these relationships provide a useful starting point for building regression models to predict car ownership. It's important to consider these insights in the context of the domain when selecting features for modeling and interpreting the results of any predictive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b339952",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce992058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceeding with model training\n",
    "# Preprocessing\n",
    "relevant_columns = ['YearlyIncome', 'HouseOwnerFlag', 'TotalChildren', \n",
    "                    'NumberChildrenAtHome', 'CommuteDistanceEncoded', 'NumberCarsOwned']\n",
    "\n",
    "# Note: Make sure 'CommuteDistanceEncoded' is treated as numerical if it's ordinal\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['YearlyIncome', 'TotalChildren', 'NumberChildrenAtHome', 'CommuteDistanceEncoded'])\n",
    "    ])\n",
    "\n",
    "# Separating features and target variable\n",
    "X = dim_customer_df[relevant_columns[:-1]]  # Exclude 'NumberCarsOwned' from features\n",
    "y = dim_customer_df['NumberCarsOwned']\n",
    "\n",
    "# Applying transformations\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Feature Importance Analysis\n",
    "# Get the feature names after transformation\n",
    "feature_names = preprocessor.named_transformers_['num'].get_feature_names_out()\n",
    "\n",
    "# Combine feature names and coefficients from the model\n",
    "feature_importances = pd.DataFrame(zip(feature_names, lr_model.coef_), \n",
    "                                   columns=['Feature', 'Coefficient'])\n",
    "\n",
    "# Sort the features by the absolute magnitude of their coefficients\n",
    "feature_importances['Absolute Coefficient'] = feature_importances['Coefficient'].abs()\n",
    "feature_importances = feature_importances.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Output the performance metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R):\", r2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2af8c1",
   "metadata": {},
   "source": [
    "Here's a step-by-step explanation of the code and its components:\n",
    "\n",
    "**Preprocessing:**\n",
    "\n",
    "relevant_columns are defined, which include both features and the target variable 'NumberCarsOwned'.\n",
    "A ColumnTransformer with StandardScaler is set up to standardize the numerical features, which include 'YearlyIncome', 'TotalChildren', 'NumberChildrenAtHome', and 'CommuteDistanceEncoded'. Standardization is a common preprocessing step that scales the data so that it has a mean of 0 and a standard deviation of 1, helping to ensure that the scale of the features does not unduly influence the model.\n",
    "Feature-Target Split:\n",
    "\n",
    "The features (X) and the target variable (y) are separated. X includes all the relevant columns except for the target variable 'NumberCarsOwned'.\n",
    "Data Transformation:\n",
    "\n",
    "The features are transformed using the preprocessor, which standardizes the numerical features.\n",
    "Train-Test Split:\n",
    "\n",
    "The dataset is split into training and testing sets using train_test_split. This allows the model to be trained on one portion of the data and then evaluated on a separate portion to test its performance on unseen data. The test size is set to 30% of the data, with the random state fixed at 42 for reproducibility.\n",
    "Model Training:\n",
    "\n",
    "A LinearRegression model is instantiated and fitted to the training data. This model learns the relationship between the features and the target variable.\n",
    "Feature Importance Analysis:\n",
    "\n",
    "After training, the feature names are retrieved and combined with the model's coefficients, which represent the influence of each standardized feature on the target variable. The absolute values of these coefficients are calculated to assess the magnitude of the feature's impact, regardless of the direction (positive or negative).\n",
    "Prediction and Evaluation:\n",
    "\n",
    "The model is used to predict the target variable for the test set.\n",
    "The model's performance is evaluated using two metrics: Root Mean Squared Error (RMSE) and R-squared (R). RMSE measures the average magnitude of the errors between the predicted and actual values; a lower RMSE indicates better fit. R measures the proportion of the variance in the target variable that can be explained by the model's features; an R of 1 indicates perfect prediction, while an R of 0 indicates that the model explains none of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88c30b",
   "metadata": {},
   "source": [
    "**Analysis of Output:**\n",
    "\n",
    "RMSE (0.8984): This value indicates the standard deviation of the prediction errors. In this context, it means that the model's predictions are, on average, 0.8984 units away from the actual number of cars owned. Whether this is acceptable depends on the context and the range of 'NumberCarsOwned' in the dataset.\n",
    "\n",
    "R (0.3765): This value suggests that approximately 37.65% of the variance in 'NumberCarsOwned' can be explained by the model. This is a moderate amount of variance explained, indicating the model has some predictive power, but there is still a significant amount of variance that the model does not account for.\n",
    "\n",
    "The analysis implies that while the model has learned some of the underlying patterns in the data, there's room for improvement. Potential steps to improve the model could include feature engineering, considering non-linear models, or addressing potential outliers and leverage points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18154804",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rounded = np.rint(y_pred)  # Round predictions to the nearest integer\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_rounded)\n",
    "\n",
    "# Output the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2452b",
   "metadata": {},
   "source": [
    "**Class-wise Breakdown:**\n",
    "\n",
    "Class 0 (First Row): 55 correct predictions, but 984 instances were incorrectly predicted as Class 1, 167 as Class 2, 7 as Class 3.\n",
    "Class 1 (Second Row): 1029 correct predictions, but 21 instances were incorrectly predicted as Class 0, 406 as Class 2, 22 as Class 3.\n",
    "Class 2 (Third Row): 799 correct predictions, with significant misclassifications into Classes 1 and 3.\n",
    "Class 3 and Class 4 (Fourth and Fifth Rows): These classes have a higher number of misclassifications compared to correct predictions.\n",
    "Implications:\n",
    "\n",
    "The model seems to struggle most with accurately predicting Class 0, often confusing it with Class 1.\n",
    "There is a considerable amount of confusion between Classes 1 and 2, and Classes 3 and 4.\n",
    "The higher classes (like 3 and 4) have relatively few correct predictions, which might be due to a lower number of instances of these classes in the dataset (class imbalance).\n",
    "\n",
    "**Possible Actions:**\n",
    "\n",
    "Improve Class Imbalance: If there's a class imbalance in the dataset, techniques like oversampling the minority class, undersampling the majority class, or using class weights can help.\n",
    "Feature Engineering: Look into creating or transforming features that might help the model distinguish between classes better.\n",
    "Model Complexity: Consider whether a more complex model or different algorithms might capture the nuances between classes better.\n",
    "Hyperparameter Tuning: Experiment with different hyperparameters to see if the model's performance can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5453bc9f",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis:\n",
    "After training the model, checking the feature importances or coefficients to see which features are having the most impact on predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying feature importances\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9073f11",
   "metadata": {},
   "source": [
    "**Analyzing the feature importances:**\n",
    "\n",
    "CommuteDistanceEncoded (Coefficient: 0.344798): This feature has the highest coefficient in absolute terms, which indicates that among the features used, 'CommuteDistanceEncoded' has the strongest relationship with the number of cars owned. Since this is a positive coefficient, a longer commute distance is associated with owning more cars.\n",
    "\n",
    "YearlyIncome (Coefficient: 0.292740): 'YearlyIncome' is also a strong predictor of the number of cars owned, with the second-highest coefficient. The positive sign means that as yearly income increases, the number of cars owned tends to increase.\n",
    "\n",
    "NumberChildrenAtHome (Coefficient: 0.272344): This feature has the third-highest impact on the number of cars owned. The positive coefficient suggests that households with more children at home are likely to own more cars.\n",
    "\n",
    "TotalChildren (Coefficient: 0.063826): 'TotalChildren' has the smallest positive impact on the number of cars owned among the features listed. This indicates a weaker relationship compared to the other features.\n",
    "'CommuteDistanceEncoded', 'YearlyIncome', and 'NumberChildrenAtHome' are the most significant predictors of car ownership in the model. The magnitude of these coefficients suggests that they are important factors to consider when predicting the number of cars a person or household owns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2101a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs. Predicted plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], '--k')  # Diagonal line\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee95d3f",
   "metadata": {},
   "source": [
    "**Actual vs. Predicted Plot:**\n",
    "\n",
    "This plot displays the actual values of the target variable on the x-axis and the predicted values on the y-axis.\n",
    "Ideal predictions would fall on the dashed diagonal line where the actual and predicted values are equal.\n",
    "The plot shows that for most actual values, there are clusters of predicted values. This indicates that for any given actual value of 'NumberCarsOwned', there is a range of predictions, which suggests some inaccuracy in the model.\n",
    "The clustering also suggests that the model may have difficulty distinguishing between different levels of the target variable, especially since the data points for actual values of 1, 2, and 3 cars seem to have overlapping predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12087c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='red', linestyles='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd336586",
   "metadata": {},
   "source": [
    "**Residuals Plot:**\n",
    "\n",
    "The residuals plot shows the residuals (the differences between the actual and predicted values) on the y-axis against the predicted values on the x-axis.\n",
    "Ideally, residuals should be randomly scattered around the horizontal line at zero, indicating that the model's predictions are consistent across all values.\n",
    "However, in this plot, we observe a clear pattern where the residuals do not scatter randomly but instead form distinct bands. This indicates systematic errors in the model's predictions.\n",
    "The bands also suggest that the relationship between the predictors and the target may not be linear or that there may be other variables affecting the outcome that are not included in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f7d89",
   "metadata": {},
   "source": [
    "**Thats the End. Thank you** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f0024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
